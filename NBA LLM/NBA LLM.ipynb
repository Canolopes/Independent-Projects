{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an NBA Large-Language-Model using Falcon Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import default_data_collator\n",
    "import re\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, BitsAndBytesConfig, default_data_collator\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, ensure proper initialization\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Reset widget state\n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n",
      "GPU device name: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"GPU available:\", torch.cuda.is_available())\n",
    "print(\"GPU device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_nba_data():\n",
    "    urls = [\n",
    "        # Advanced Stats\n",
    "        \"https://www.basketball-reference.com/leaders/per_career.html\",\n",
    "        \"https://www.basketball-reference.com/leaders/ws_career.html\", \n",
    "        \"https://www.basketball-reference.com/leaders/bpm_career.html\",\n",
    "        \"https://www.basketball-reference.com/leaders/vorp_career.html\",\n",
    "        \n",
    "        # All-Time Great Teams\n",
    "        \"https://www.basketball-reference.com/teams/CHI/1996.html\",\n",
    "        \"https://www.basketball-reference.com/teams/GSW/2016.html\", \n",
    "        \"https://www.basketball-reference.com/teams/LAL/1972.html\",\n",
    "        \"https://www.basketball-reference.com/teams/BOS/1986.html\",\n",
    "        \n",
    "        # Hall of Fame Players\n",
    "        \"https://www.basketball-reference.com/players/r/russebi01.html\",\n",
    "        \"https://www.basketball-reference.com/players/o/olajuha01.html\",\n",
    "        \"https://www.basketball-reference.com/players/m/malonka01.html\",\n",
    "        \"https://www.basketball-reference.com/players/e/ervinju01.html\",\n",
    "        \n",
    "        # Season Leaders\n",
    "        \"https://www.basketball-reference.com/leaders/pts_season.html\",\n",
    "        \"https://www.basketball-reference.com/leaders/ast_season.html\",\n",
    "        \"https://www.basketball-reference.com/leaders/reb_season.html\",\n",
    "        \"https://www.basketball-reference.com/leaders/blk_season.html\",\n",
    "        \n",
    "        # All-Star Games\n",
    "        \"https://www.basketball-reference.com/allstar/\",\n",
    "        \"https://www.basketball-reference.com/allstar/NBA_2023.html\",\n",
    "        \n",
    "        # Historical Seasons\n",
    "        \"https://www.basketball-reference.com/leagues/NBA_1996.html\",\n",
    "        \"https://www.basketball-reference.com/leagues/NBA_1986.html\",\n",
    "        \"https://www.basketball-reference.com/leagues/NBA_1972.html\",\n",
    "        \n",
    "        # Additional Open Sources\n",
    "        \"https://www.landofbasketball.com/records/\",\n",
    "        \"https://www.landofbasketball.com/all_time_leaders/\",\n",
    "        \"https://www.landofbasketball.com/year_by_year.htm\",\n",
    "        \"https://hoopshype.com/salaries/\",\n",
    "        \"https://www.realgm.com/nba/stats/\",\n",
    "        \"https://www.proballers.com/basketball/\",\n",
    "        \"https://www.usbasket.com/\",\n",
    "        \"https://basketball.realgm.com/nba/awards/\"\n",
    "    ]\n",
    "    \n",
    "    all_data = []\n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        stats = extract_stats(soup)\n",
    "        cleaned_stats = clean_training_data(stats)\n",
    "        all_data.append(cleaned_stats)\n",
    "    \n",
    "    return \"\\n\\n\".join(all_data)\n",
    "\n",
    "def clean_training_data(text):\n",
    "    cleaned = re.sub(r'(\\* Indicates.*?\\n)(?=\\* Indicates)', '', text, flags=re.MULTILINE)\n",
    "    cleaned = re.sub(r'(Active players.*?\\n)(?=Active players)', '', cleaned, flags=re.MULTILINE)\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "    return cleaned.strip()\n",
    "\n",
    "def extract_stats(soup):\n",
    "    stats_text = \"NBA Career Scoring Leaders:\\n\"\n",
    "    \n",
    "    # Extract scoring table with clear formatting\n",
    "    scoring_leaders = soup.find('table', {'id': 'leaders'})\n",
    "    if scoring_leaders:\n",
    "        rows = scoring_leaders.find_all('tr')\n",
    "        for row in rows[1:]:  # Skip header\n",
    "            cols = row.find_all('td')\n",
    "            if cols:\n",
    "                rank = cols[0].text.strip()\n",
    "                player = cols[1].text.strip()\n",
    "                points = cols[2].text.strip()\n",
    "                stats_text += f\"Rank {rank}: {player} - {points} points\\n\"\n",
    "    \n",
    "    # Add contextual sentences\n",
    "    stats_text += \"\\nKey NBA Scoring Milestones:\\n\"\n",
    "    stats_text += \"LeBron James broke the all-time scoring record on February 7, 2023\\n\"\n",
    "    stats_text += \"Previous record holder was Kareem Abdul-Jabbar with 38,387 points\\n\"\n",
    "    \n",
    "    return stats_text\n",
    "\n",
    "scraped_data = scrape_nba_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d866ca40ccc4006ac0444018a7ffc11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the scraped data to a text file\n",
    "scraped_data = scrape_nba_data()\n",
    "with open(\"nba_data.txt\", \"w\", encoding='utf-8') as f:\n",
    "    f.write(scraped_data)\n",
    "\n",
    "# Step 2: Preprocessing the Data\n",
    "def preprocess_data(file_path):\n",
    "    \"\"\"Loads and preprocesses the text data.\"\"\"\n",
    "    with open(file_path, \"r\", encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "preprocessed_text = preprocess_data(\"nba_data.txt\")\n",
    "\n",
    "# Convert to a dataset format\n",
    "data_dict = {\"text\": [preprocessed_text]}\n",
    "dataset = Dataset.from_dict(data_dict)\n",
    "\n",
    "# Set up quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Step 3: Fine-Tuning the Model\n",
    "model_name = \"tiiuae/falcon-7b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    use_cache=False,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={'': torch.cuda.current_device()},  # Forces GPU-only\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "290e653358ff4c87b95dc51eafc9c7b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Update tokenization function to return proper tensor format\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=512,\n",
    "        return_tensors=None  # Important: let the collator handle tensors\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory optimization settings\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:512'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\canol\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 01:15, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.265100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.264000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.260700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.256800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.253100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.250100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.248700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.247600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.246900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.246600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=0.25395263671875, metrics={'train_runtime': 78.5572, 'train_samples_per_second': 1.273, 'train_steps_per_second': 1.273, 'total_flos': 2127077376000000.0, 'train_loss': 0.25395263671875, 'epoch': 100.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query_key_value\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=[\"lm_head\"]\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=1e-6,\n",
    "    logging_steps=10,\n",
    "    optim='adamw_torch_fused',\n",
    "    dataloader_pin_memory=False,\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=100,\n",
    "    warmup_ratio=0.1, \n",
    "    weight_decay=0.05,\n",
    "    gradient_accumulation_steps=32,\n",
    "    gradient_checkpointing=True,\n",
    "    torch_compile=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./nba_llm\\\\tokenizer_config.json',\n",
       " './nba_llm\\\\special_tokens_map.json',\n",
       " './nba_llm\\\\tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Save the Model\n",
    "model.save_pretrained(\"./nba_llm\")\n",
    "tokenizer.save_pretrained(\"./nba_llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\canol\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:676: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "C:\\Users\\canol\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who has the most rebounds in NBA history?\n",
      "Answer: Who has the most rebounds in NBA history?\n",
      "Wilt Chamberlain\n",
      "Who has most rebounds NBA?\n",
      "The NBAâ€™s all-time leader in rebounds is Wilt Chamberlin with\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(prompt, model, tokenizer, max_length=128):\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    ).to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=30, \n",
    "        do_sample=True,\n",
    "        temperature=0.1,  \n",
    "        top_p=0.7, \n",
    "        top_k=50,      \n",
    "        repetition_penalty=1.2, \n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        no_repeat_ngram_size=3,  \n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    # Clean and format the response\n",
    "    response = tokenizer.decode(\n",
    "        outputs[0], \n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    ).strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "prompt = \"Who has the most rebounds in NBA history?\"\n",
    "response = evaluate_model(prompt, model, tokenizer)\n",
    "print(f\"Question: {prompt}\\nAnswer: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who has the most points in NBA history?\n",
      "Answer: Who has the most points in NBA history?\n",
      "Michael Jordan\n",
      "Who has scored the most NBA points?\n",
      "Kareem Abdul-Jabbar\n",
      "Who is the highest scorer in NBA?\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Who has the most points in NBA history?\"\n",
    "response = evaluate_model(prompt, model, tokenizer)\n",
    "print(f\"Question: {prompt}\\nAnswer: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who has the most 3-pointers in NBA history?\n",
      "Answer: Who has the most 3-pointers in NBA history?\n",
      "The NBA record for 3 pointers is held by Ray Allen with 2,973.\n",
      "Who has made the most NBA 3 point shots\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Who has the most 3-pointers in NBA history?\"\n",
    "response = evaluate_model(prompt, model, tokenizer)\n",
    "print(f\"Question: {prompt}\\nAnswer: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
